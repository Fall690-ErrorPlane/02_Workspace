{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491b904e",
   "metadata": {},
   "source": [
    "# Introduction to SageMaker TensorFlow - Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec86c6d-2b6d-4332-bddd-b0971a9f821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://raw.githubusercontent.com/aws/amazon-sagemaker-examples/main/introduction_to_amazon_algorithms/image_classification_tensorflow/Amazon_TensorFlow_Image_Classification.ipynb\n",
    "#tutorial https://aws.amazon.com/blogs/machine-learning/classify-your-own-images-using-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df953a",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to [Amazon SageMaker Built-in Algorithms](https://sagemaker.readthedocs.io/en/stable/algorithms/index.html)! You can use SageMaker Built-in algorithms to solve many Machine Learning tasks through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html). You can also use these algorithms through one-click in SageMaker Studio via [JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html).\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the TensorFlow Image Classification algorithm. Image Classification refers to classifying an image to one of the class labels of the training dataset.  We demonstrate two use cases of TensorFlow Image Classification models:\n",
    "\n",
    "* How to use a model pre-trained on ImageNet dataset to classify an image. [ImageNetLabels](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt).\n",
    "* How to fine-tune a pre-trained model to a custom dataset, and then run inference on the fine-tuned model.\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf3836",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. Tutorial section not needed and removed for simplicity\n",
    "4. [Fine-tune the pre-trained model on a custom dataset](#4.-Fine-tune-the-pre-trained-model-on-a-custome-dataset)\n",
    "    * [Retrieve Training artifacts](#4.1.-Retrieve-Training-artifacts)\n",
    "    * [Set Training parameters](#4.2.-Set-Training-parameters)\n",
    "    * [Train with Automatic Model Tuning (HPO)](#AMT)\n",
    "    * [Start Training](#4.4.-Start-Training)\n",
    "    * [Extract Training performance metrics](#4.5.-Extract-Training-performance-metrics)\n",
    "    * [Deploy & run Inference on the fine-tuned model](#4.6.-Deploy-&-run-Inference-on-the-fine-tuned-model)\n",
    "    * [Incrementally train the fine-tuned model](#4.7.-Incrementally-train-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e04731",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "***\n",
    "Before executing the notebook, there are some initial steps required for setup. This notebook requires latest version of sagemaker and ipywidgets.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a536a5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e8b8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab99140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dd01d",
   "metadata": {},
   "source": [
    "## 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at [Sagemaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f1d777",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ]
   },
   "outputs": [],
   "source": [
    "#model_id, model_version = \"tensorflow-ic-imagenet-mobilenet-v2-100-224-classification-4\", \"*\"\n",
    "model_id, model_version =\"tensorflow-ic-resnet-50-classification-1\", \"*\"\n",
    "#model_id, model_version =\"tensorflow-ic-efficientnet-b0-classification-1\", \"*\"\n",
    "#model_id, model_version =\"tensorflow-ic-tf2-preview-inception-v3-classification-4\", \"*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772154b7",
   "metadata": {},
   "source": [
    "***\n",
    "[Optional] Select a different Sagemaker pre-trained model. Here, we download the model_manifest file from the Built-In Algorithms s3 bucket, filter-out all the Image Classification models and select a model for inference.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7cb33f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Select a SageMaker pre-trained model from the dropdown below"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867bb420613e4fb0b96eba855f97ce2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='SageMaker Built-In TensorFlow Image Classification Models:', index=57, layout=Layout(widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import IPython\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And\n",
    "\n",
    "# Retrieves all TensorFlow Image Classification models made available by SageMaker Built-In Algorithms.\n",
    "filter_value = And(\"task == ic\", \"framework == tensorflow\")\n",
    "ic_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown, for user to select a model.\n",
    "dropdown = Dropdown(\n",
    "    options=ic_models,\n",
    "    value=model_id,\n",
    "    description=\"SageMaker Built-In TensorFlow Image Classification Models:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(IPython.display.Markdown(\"## Select a SageMaker pre-trained model from the dropdown below\"))\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfa6a72-550a-4586-8860-4c83e2d91c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensorflow-ic-bit-m-r101x1-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-m-r101x1-imagenet21k-classification-1',\n",
       " 'tensorflow-ic-bit-m-r101x3-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-m-r101x3-imagenet21k-classification-1',\n",
       " 'tensorflow-ic-bit-m-r50x1-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-m-r50x1-imagenet21k-classification-1',\n",
       " 'tensorflow-ic-bit-m-r50x3-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-m-r50x3-imagenet21k-classification-1',\n",
       " 'tensorflow-ic-bit-s-r101x1-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-s-r101x3-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-s-r50x1-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-bit-s-r50x3-ilsvrc2012-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b0-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b1-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b2-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b3-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b4-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b5-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b6-classification-1',\n",
       " 'tensorflow-ic-efficientnet-b7-classification-1',\n",
       " 'tensorflow-ic-efficientnet-lite0-classification-2',\n",
       " 'tensorflow-ic-efficientnet-lite1-classification-2',\n",
       " 'tensorflow-ic-efficientnet-lite2-classification-2',\n",
       " 'tensorflow-ic-efficientnet-lite3-classification-2',\n",
       " 'tensorflow-ic-efficientnet-lite4-classification-2',\n",
       " 'tensorflow-ic-imagenet-inception-resnet-v2-classification-4',\n",
       " 'tensorflow-ic-imagenet-inception-v1-classification-4',\n",
       " 'tensorflow-ic-imagenet-inception-v2-classification-4',\n",
       " 'tensorflow-ic-imagenet-inception-v3-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-025-128-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-025-160-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-025-192-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-025-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-050-128-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-050-160-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-050-192-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-050-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-075-128-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-075-160-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-075-192-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-075-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-100-128-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-100-160-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-100-192-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v1-100-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-035-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-050-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-075-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-100-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-130-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-mobilenet-v2-140-224-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v1-101-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v1-152-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v1-50-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v2-101-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v2-152-classification-4',\n",
       " 'tensorflow-ic-imagenet-resnet-v2-50-classification-4',\n",
       " 'tensorflow-ic-resnet-50-classification-1',\n",
       " 'tensorflow-ic-tf2-preview-inception-v3-classification-4',\n",
       " 'tensorflow-ic-tf2-preview-mobilenet-v2-classification-4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504466ea",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the pre-trained model on a custom dataset\n",
    "***\n",
    "Previously, we saw how to run inference on a pre-trained model. Next, we discuss how a model can be fine-tuned to a custom dataset with any number of classes. \n",
    "\n",
    "The model available for fine-tuning attaches a classification layer to the corresponding feature extractor model available on TensorFlow/PyTorch hub, and initializes the layer parameters to random values. The output dimension of the classification layer is determined based on the number of classes in the input data. The fine-tuning step fine-tunes the model parameters. The objective is to minimize classification error on the input data. The model returned by fine-tuning can be further deployed for inference. Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A directory with as many sub-directories as the number of classes. \n",
    "    - Each sub-directory should have images belonging to that class in .jpg format. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "    - A label mapping file is saved along with the trained model file on the s3 bucket.  \n",
    "    \n",
    "The input directory should look like below if the training data contains images from two classes: roses and dandelion. The s3 path should look like `s3://bucket_name/input_directory/`. Note the trailing `/` is required. The names of the folders and 'roses', 'dandelion', and the .jpg filenames can be anything. The label mapping file that is saved along with the trained model on the s3 bucket maps the folder names 'roses' and 'dandelion' to the indices in the list of class probabilities the model outputs. The mapping follows alphabetical ordering of the folder names. In the example below, index 0 in the model output list would correspond to 'dandelion' and index 1 would correspond to 'roses'.\n",
    "\n",
    "    input_directory\n",
    "        |--roses\n",
    "            |--abc.jpg\n",
    "            |--def.jpg\n",
    "        |--dandelion\n",
    "            |--ghi.jpg\n",
    "            |--jkl.jpg\n",
    "\n",
    "We provide tf_flowers dataset as a default dataset for fine-tuning the model. tf_flower comprises images of five types of flowers. The dataset has been downloaded from [TensorFlow](https://www.tensorflow.org/datasets/catalog/tf_flowers) under [Apache 2.0 License](https://jumpstart-cache-prod-us-west-2.s3-us-west-2.amazonaws.com/licenses/Apache-License/LICENSE-2.0.txt).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2c89a",
   "metadata": {},
   "source": [
    "### 4.1. Retrieve Training artifacts\n",
    "***\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained base model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the lates model. Also, we do need to specify the training_instance_type to fetch train_image_uri.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d47fb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "model_id, model_version = dropdown.value, \"*\"\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de193b7b-a3eb-472c-8d04-0827e212cd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-ic-resnet-50-classification-1'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cbb5b",
   "metadata": {},
   "source": [
    "### 4.2. Set Training parameters\n",
    "***\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Image Classification model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a6897f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "#training_data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "#training_data_prefix = \"training-datasets/tf_flowers/\"\n",
    "\n",
    "#training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "#training_dataset_s3_path = f\"s3://contrail-classification-input-101022\"\n",
    "#training_dataset_s3_path = f\"s3://contrail-classification-input-101522\"\n",
    "training_dataset_s3_path = f\"s3://contrail-classification-input-b4\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-ic-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410123e7",
   "metadata": {},
   "source": [
    "***\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4265a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_only_top_layer': 'True', 'epochs': '25', 'batch_size': '32', 'optimizer': 'adam', 'learning_rate': '0.001', 'beta_1': '0.9', 'beta_2': '0.999', 'momentum': '0.9', 'epsilon': '1e-07', 'rho': '0.95', 'initial_accumulator_value': '0.1', 'reinitialize_top_layer': 'Auto', 'early_stopping': 'False', 'early_stopping_patience': '5', 'early_stopping_min_delta': '0.0', 'dropout_rate': '0.2', 'regularizers_l2': '0.0001', 'label_smoothing': '0.1', 'image_resize_interpolation': 'bilinear', 'augmentation': 'False', 'augmentation_random_flip': 'horizontal_and_vertical', 'augmentation_random_rotation': '0.2', 'augmentation_random_zoom': '0.1', 'binary_mode': 'False', 'eval_metric': 'accuracy', 'validation_split_ratio': '0.2'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "#hyperparameters[\"epochs\"] = \"5\"\n",
    "hyperparameters[\"epochs\"] = \"25\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095df25",
   "metadata": {},
   "source": [
    "### 4.3. Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "147d2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False \n",
    "#use_amt = True #RVM update 10/13 for better metric?\n",
    "\n",
    "# Define objective metric per framework, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "    \"type\": \"Maximize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"adam-learning-rate\": ContinuousParameter(0.0001, 0.1, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3011dd2",
   "metadata": {},
   "source": [
    "### 4.4. Start Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5068463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-30 12:17:55 Starting - Starting the training job...\n",
      "2022-10-30 12:18:22 Starting - Preparing the instances for trainingProfilerReport-1667132275: InProgress\n",
      ".........\n",
      "2022-10-30 12:19:49 Downloading - Downloading input data...\n",
      "2022-10-30 12:20:20 Training - Downloading the training image.....................\n",
      "2022-10-30 12:23:51 Training - Training image download completed. Training in progress..\u001b[34m2022-10-30 12:23:55.096646: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:23:55.107682: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:23:55.456390: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:24:00,956 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-10-30 12:24:01,665 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"augmentation\": \"False\",\n",
      "        \"augmentation_random_flip\": \"horizontal_and_vertical\",\n",
      "        \"augmentation_random_rotation\": \"0.2\",\n",
      "        \"augmentation_random_zoom\": \"0.1\",\n",
      "        \"batch_size\": \"32\",\n",
      "        \"beta_1\": \"0.9\",\n",
      "        \"beta_2\": \"0.999\",\n",
      "        \"binary_mode\": \"False\",\n",
      "        \"dropout_rate\": \"0.2\",\n",
      "        \"early_stopping\": \"False\",\n",
      "        \"early_stopping_min_delta\": \"0.0\",\n",
      "        \"early_stopping_patience\": \"5\",\n",
      "        \"epochs\": \"25\",\n",
      "        \"epsilon\": \"1e-07\",\n",
      "        \"eval_metric\": \"accuracy\",\n",
      "        \"image_resize_interpolation\": \"bilinear\",\n",
      "        \"initial_accumulator_value\": \"0.1\",\n",
      "        \"label_smoothing\": \"0.1\",\n",
      "        \"learning_rate\": \"0.001\",\n",
      "        \"momentum\": \"0.9\",\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"regularizers_l2\": \"0.0001\",\n",
      "        \"reinitialize_top_layer\": \"Auto\",\n",
      "        \"rho\": \"0.95\",\n",
      "        \"train_only_top_layer\": \"True\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"jumpstart-example-tensorflow-ic-resnet--2022-10-30-12-17-55-060\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/ic/v2.0.4/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"augmentation\":\"False\",\"augmentation_random_flip\":\"horizontal_and_vertical\",\"augmentation_random_rotation\":\"0.2\",\"augmentation_random_zoom\":\"0.1\",\"batch_size\":\"32\",\"beta_1\":\"0.9\",\"beta_2\":\"0.999\",\"binary_mode\":\"False\",\"dropout_rate\":\"0.2\",\"early_stopping\":\"False\",\"early_stopping_min_delta\":\"0.0\",\"early_stopping_patience\":\"5\",\"epochs\":\"25\",\"epsilon\":\"1e-07\",\"eval_metric\":\"accuracy\",\"image_resize_interpolation\":\"bilinear\",\"initial_accumulator_value\":\"0.1\",\"label_smoothing\":\"0.1\",\"learning_rate\":\"0.001\",\"momentum\":\"0.9\",\"optimizer\":\"adam\",\"regularizers_l2\":\"0.0001\",\"reinitialize_top_layer\":\"Auto\",\"rho\":\"0.95\",\"train_only_top_layer\":\"True\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/ic/v2.0.4/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"augmentation\":\"False\",\"augmentation_random_flip\":\"horizontal_and_vertical\",\"augmentation_random_rotation\":\"0.2\",\"augmentation_random_zoom\":\"0.1\",\"batch_size\":\"32\",\"beta_1\":\"0.9\",\"beta_2\":\"0.999\",\"binary_mode\":\"False\",\"dropout_rate\":\"0.2\",\"early_stopping\":\"False\",\"early_stopping_min_delta\":\"0.0\",\"early_stopping_patience\":\"5\",\"epochs\":\"25\",\"epsilon\":\"1e-07\",\"eval_metric\":\"accuracy\",\"image_resize_interpolation\":\"bilinear\",\"initial_accumulator_value\":\"0.1\",\"label_smoothing\":\"0.1\",\"learning_rate\":\"0.001\",\"momentum\":\"0.9\",\"optimizer\":\"adam\",\"regularizers_l2\":\"0.0001\",\"reinitialize_top_layer\":\"Auto\",\"rho\":\"0.95\",\"train_only_top_layer\":\"True\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"jumpstart-example-tensorflow-ic-resnet--2022-10-30-12-17-55-060\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/ic/v2.0.4/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--augmentation\",\"False\",\"--augmentation_random_flip\",\"horizontal_and_vertical\",\"--augmentation_random_rotation\",\"0.2\",\"--augmentation_random_zoom\",\"0.1\",\"--batch_size\",\"32\",\"--beta_1\",\"0.9\",\"--beta_2\",\"0.999\",\"--binary_mode\",\"False\",\"--dropout_rate\",\"0.2\",\"--early_stopping\",\"False\",\"--early_stopping_min_delta\",\"0.0\",\"--early_stopping_patience\",\"5\",\"--epochs\",\"25\",\"--epsilon\",\"1e-07\",\"--eval_metric\",\"accuracy\",\"--image_resize_interpolation\",\"bilinear\",\"--initial_accumulator_value\",\"0.1\",\"--label_smoothing\",\"0.1\",\"--learning_rate\",\"0.001\",\"--momentum\",\"0.9\",\"--optimizer\",\"adam\",\"--regularizers_l2\",\"0.0001\",\"--reinitialize_top_layer\",\"Auto\",\"--rho\",\"0.95\",\"--train_only_top_layer\",\"True\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_AUGMENTATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_AUGMENTATION_RANDOM_FLIP=horizontal_and_vertical\u001b[0m\n",
      "\u001b[34mSM_HP_AUGMENTATION_RANDOM_ROTATION=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_AUGMENTATION_RANDOM_ZOOM=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_BETA_2=0.999\u001b[0m\n",
      "\u001b[34mSM_HP_BINARY_MODE=False\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT_RATE=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING=False\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_MIN_DELTA=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_PATIENCE=5\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=25\u001b[0m\n",
      "\u001b[34mSM_HP_EPSILON=1e-07\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_METRIC=accuracy\u001b[0m\n",
      "\u001b[34mSM_HP_IMAGE_RESIZE_INTERPOLATION=bilinear\u001b[0m\n",
      "\u001b[34mSM_HP_INITIAL_ACCUMULATOR_VALUE=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_SMOOTHING=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_MOMENTUM=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGULARIZERS_L2=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_REINITIALIZE_TOP_LAYER=Auto\u001b[0m\n",
      "\u001b[34mSM_HP_RHO=0.95\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_ONLY_TOP_LAYER=True\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python39.zip:/usr/local/lib/python3.9:/usr/local/lib/python3.9/lib-dynload:/usr/local/lib/python3.9/site-packages:/usr/local/lib/python3.9/site-packages/smdebug-1.0.14b20221003-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument-3.4.2-py3.9.egg:/usr/local/lib/python3.9/site-packages/pyinstrument_cext-0.2.4-py3.9-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.9 transfer_learning.py --augmentation False --augmentation_random_flip horizontal_and_vertical --augmentation_random_rotation 0.2 --augmentation_random_zoom 0.1 --batch_size 32 --beta_1 0.9 --beta_2 0.999 --binary_mode False --dropout_rate 0.2 --early_stopping False --early_stopping_min_delta 0.0 --early_stopping_patience 5 --epochs 25 --epsilon 1e-07 --eval_metric accuracy --image_resize_interpolation bilinear --initial_accumulator_value 0.1 --label_smoothing 0.1 --learning_rate 0.001 --momentum 0.9 --optimizer adam --regularizers_l2 0.0001 --reinitialize_top_layer Auto --rho 0.95 --train_only_top_layer True --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34mExtension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\u001b[0m\n",
      "\u001b[34mIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\u001b[0m\n",
      "\u001b[34mWarning! MPI libs are missing, but python applications are still available.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:24:02.543337: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:24:02.543556: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:24:02.597486: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mRunning training scripts with arguments: Namespace(model_dir='/opt/ml/model', train='/opt/ml/input/data/training', validation=None, pretrained_model='/opt/ml/input/data/model', hosts=['algo-1'], current_host='algo-1', verbose_one_line_per_epoch=2, checkpoint_save_best_only='True', seed=0, reinitialize_top_layer='Auto', train_only_top_layer='True', validation_split_ratio=0.2, early_stopping='False', early_stopping_patience=5, early_stopping_min_delta=0.0, dropout_rate=0.2, regularizers_l2=0.0001, label_smoothing=0.1, image_resize_interpolation='bilinear', augmentation='False', augmentation_random_flip='horizontal_and_vertical', augmentation_random_rotation=0.2, augmentation_random_zoom=0.1, epochs=25, batch_size=32, optimizer='adam', learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, momentum=0.9, rho=0.95, initial_accumulator_value=0.1, binary_mode='False', eval_metric='accuracy')\u001b[0m\n",
      "\u001b[34mIgnoring unrecognized arguments: ['--validation_split_ratio', '0.2']\u001b[0m\n",
      "\u001b[34mimage size for this model is: [224, 224]\u001b[0m\n",
      "\u001b[34mFor labels using categorical mode.\u001b[0m\n",
      "\u001b[34mFound 425 files belonging to 2 classes.\u001b[0m\n",
      "\u001b[34mcardinality of dataset 425\u001b[0m\n",
      "\u001b[34mcardinality of train dataset: 340\u001b[0m\n",
      "\u001b[34mcardinality of validation dataset: 85\u001b[0m\n",
      "\u001b[34mprediction class indices mapping to input training data labels from index 0 to 1: ['neg', 'pos']\u001b[0m\n",
      "\u001b[34mnumber of examples for each class in train dataset: [156. 184.]\u001b[0m\n",
      "\u001b[34mnumber of examples for each class in validation dataset: [33. 52.]\u001b[0m\n",
      "\u001b[34m'finetuned' not in model info file. Assuming model was not fine-tuned previously.\u001b[0m\n",
      "\u001b[34mAttaching a randomly initialized classification layer on top of the original feature prediction model to classify input images to one of the 2 classes.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mNo training configuration found in save file, so the model was *not* compiled. Compile it manually.\u001b[0m\n",
      "\u001b[34mModel: \"sequential\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34msequential_37 (Sequential)  (None, 2048)              23561152\u001b[0m\n",
      "\u001b[34mdropout (Dropout)           (None, 2048)              0\u001b[0m\n",
      "\u001b[34mdense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 23,565,250\u001b[0m\n",
      "\u001b[34mTrainable params: 4,098\u001b[0m\n",
      "\u001b[34mNon-trainable params: 23,561,152\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mSetting the evaluation metric to val_accuracy\u001b[0m\n",
      "\u001b[34mEpoch 1/25\u001b[0m\n",
      "\u001b[34mExtension horovod.torch has not been built: /usr/local/lib/python3.9/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-39-x86_64-linux-gnu.so not found\u001b[0m\n",
      "\u001b[34mIf this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\u001b[0m\n",
      "\u001b[34mWarning! MPI libs are missing, but python applications are still available.\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.005 ip-10-0-248-40.ec2.internal:36 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.14b20221003-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/smdebug-1.0.14b20221003-py3.9.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.320 ip-10-0-248-40.ec2.internal:36 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.421 ip-10-0-248-40.ec2.internal:36 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.422 ip-10-0-248-40.ec2.internal:36 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.423 ip-10-0-248-40.ec2.internal:36 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.423 ip-10-0-248-40.ec2.internal:36 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-10-30 12:25:31.424 ip-10-0-248-40.ec2.internal:36 INFO hook.py:421] Monitoring the collections: sm_metrics, losses, metrics\u001b[0m\n",
      "\u001b[34m11/11 - 86s - loss: 1.2056 - accuracy: 0.4882 - val_loss: 1.0622 - val_accuracy: 0.4588 - 86s/epoch - 8s/step\u001b[0m\n",
      "\u001b[34mEpoch 2/25\u001b[0m\n",
      "\u001b[34m11/11 - 2s - loss: 1.0683 - accuracy: 0.5765 - val_loss: 1.0117 - val_accuracy: 0.6824 - 2s/epoch - 176ms/step\u001b[0m\n",
      "\u001b[34mEpoch 3/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 1.0450 - accuracy: 0.6000 - val_loss: 0.9748 - val_accuracy: 0.6471 - 595ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 4/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.9968 - accuracy: 0.6500 - val_loss: 0.9189 - val_accuracy: 0.6941 - 1s/epoch - 135ms/step\u001b[0m\n",
      "\u001b[34mEpoch 5/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.9422 - accuracy: 0.7000 - val_loss: 0.9135 - val_accuracy: 0.7412 - 1s/epoch - 122ms/step\u001b[0m\n",
      "\u001b[34mEpoch 6/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.9261 - accuracy: 0.7118 - val_loss: 0.8839 - val_accuracy: 0.8235 - 1s/epoch - 117ms/step\u001b[0m\n",
      "\u001b[34mEpoch 7/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.9132 - accuracy: 0.7235 - val_loss: 0.8830 - val_accuracy: 0.7765 - 590ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 8/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8840 - accuracy: 0.7618 - val_loss: 0.8631 - val_accuracy: 0.8235 - 591ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 9/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8686 - accuracy: 0.7676 - val_loss: 0.8569 - val_accuracy: 0.8353 - 1s/epoch - 124ms/step\u001b[0m\n",
      "\u001b[34mEpoch 10/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8443 - accuracy: 0.8176 - val_loss: 0.8542 - val_accuracy: 0.8235 - 618ms/epoch - 56ms/step\u001b[0m\n",
      "\u001b[34mEpoch 11/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8411 - accuracy: 0.8029 - val_loss: 0.8461 - val_accuracy: 0.8353 - 612ms/epoch - 56ms/step\u001b[0m\n",
      "\u001b[34mEpoch 12/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8142 - accuracy: 0.8235 - val_loss: 0.8390 - val_accuracy: 0.8353 - 591ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 13/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7940 - accuracy: 0.8471 - val_loss: 0.8343 - val_accuracy: 0.8235 - 587ms/epoch - 53ms/step\u001b[0m\n",
      "\u001b[34mEpoch 14/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.8165 - accuracy: 0.8382 - val_loss: 0.8456 - val_accuracy: 0.7882 - 594ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 15/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7958 - accuracy: 0.8353 - val_loss: 0.8322 - val_accuracy: 0.8353 - 590ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 16/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7704 - accuracy: 0.8794 - val_loss: 0.8402 - val_accuracy: 0.8471 - 1s/epoch - 119ms/step\u001b[0m\n",
      "\u001b[34mEpoch 17/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7629 - accuracy: 0.8794 - val_loss: 0.8261 - val_accuracy: 0.8353 - 590ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 18/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7691 - accuracy: 0.8676 - val_loss: 0.8152 - val_accuracy: 0.8471 - 593ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 19/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7633 - accuracy: 0.8618 - val_loss: 0.8183 - val_accuracy: 0.8235 - 590ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 20/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7477 - accuracy: 0.8706 - val_loss: 0.8232 - val_accuracy: 0.8118 - 593ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 21/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7348 - accuracy: 0.8941 - val_loss: 0.8088 - val_accuracy: 0.8353 - 588ms/epoch - 53ms/step\u001b[0m\n",
      "\u001b[34mEpoch 22/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7467 - accuracy: 0.8824 - val_loss: 0.8027 - val_accuracy: 0.8471 - 593ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 23/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7443 - accuracy: 0.8824 - val_loss: 0.8025 - val_accuracy: 0.8353 - 588ms/epoch - 53ms/step\u001b[0m\n",
      "\u001b[34mEpoch 24/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7249 - accuracy: 0.9029 - val_loss: 0.7983 - val_accuracy: 0.8471 - 594ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch 25/25\u001b[0m\n",
      "\u001b[34m11/11 - 1s - loss: 0.7221 - accuracy: 0.9059 - val_loss: 0.8045 - val_accuracy: 0.8471 - 591ms/epoch - 54ms/step\u001b[0m\n",
      "\u001b[34mEpoch achieving the maximum val_accuracy: 16/25\u001b[0m\n",
      "\u001b[34mEvaluating the model achieving highest val_accuracy, on validation data, at the end of epoch 16\u001b[0m\n",
      "\u001b[34m3/3 - 0s - loss: 0.8402 - accuracy: 0.8471 - 134ms/epoch - 45ms/step\u001b[0m\n",
      "\u001b[34mSaving the model with the highest val_accuracy, at the end of epoch 16, for running inference or incremental training.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mFrom /usr/local/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:616: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34m2022-10-30 12:27:25.919556: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34mAssets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34m2022-10-30 12:27:42,960 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:27:42,961 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-30 12:27:42,962 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-30 12:28:01 Uploading - Uploading generated training model\n",
      "2022-10-30 12:28:22 Completed - Training job completed\n",
      "ProfilerReport-1667132275: NoIssuesFound\n",
      "Training seconds: 508\n",
      "Billable seconds: 508\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "training_metric_definitions = [\n",
    "    {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_accuracy\", \"Regex\": \"- accuracy: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"- loss: ([0-9\\\\.]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "ic_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=training_metric_definitions,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        ic_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"training\": training_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    ic_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e75e44c",
   "metadata": {},
   "source": [
    "### 4.5. Extract Training performance metrics, note a regex issue gives an invalid result in Section 4.5\n",
    "***\n",
    "Performance metrics such as training accuracy/loss and validation accuracy/loss can be accessed through cloudwatch while the training. Code below provides the link to the cloudwatch log where these metrics can be found. \n",
    "\n",
    "Note that default resolution in Amazon Cloudwatch is one minute i.e. it averages the metrics logged within a single minute interval. Amazon CloudWatch also supports [high-resolution custom metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html), and its finest resolution is 1 second. However, the finer the resolution, the shorter the lifespan of the CloudWatch metrics. For the 1-second frequency resolution, the CloudWatch metrics are available for 3 hours. For more information about the resolution and the lifespan of the CloudWatch metrics, see [GetMetricStatistics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_GetMetricStatistics.html) in the Amazon CloudWatch API Reference.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6120c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = ic_estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "422ac8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "CloudWatch metrics: [link](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:query=%7B/aws/sagemaker/TrainingJobs,TrainingJobName%7D%20jumpstart-example-tensorflow-ic-resnet--2022-10-30-12-17-55-060)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "link = (\n",
    "    \"https://console.aws.amazon.com/cloudwatch/home?region=\"\n",
    "    + sagemaker_session.boto_region_name\n",
    "    + \"#metricsV2:query=%7B/aws/sagemaker/TrainingJobs,TrainingJobName%7D%20\"\n",
    "    + training_job_name\n",
    ")\n",
    "display(Markdown(\"CloudWatch metrics: [link](\" + link + \")\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15c4bb",
   "metadata": {},
   "source": [
    "***\n",
    "Alternatively, we can also fetch these metrics and analyze them within the notebook.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d915b42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>val_accuracy</td>\n",
       "      <td>3.83060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>val_loss</td>\n",
       "      <td>0.85458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.78942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train_loss</td>\n",
       "      <td>0.85798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp     metric_name    value\n",
       "0        0.0    val_accuracy  3.83060\n",
       "1        0.0        val_loss  0.85458\n",
       "2        0.0  train_accuracy  0.78942\n",
       "3        0.0      train_loss  0.85798"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ec21e-5dac-4beb-be67-b4ac6e38a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ab950",
   "metadata": {},
   "source": [
    "***\n",
    "We can filter out different metrics by names as well.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df44f7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>val_loss</td>\n",
       "      <td>0.85458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name    value\n",
       "1        0.0    val_loss  0.85458"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_names = [metric[\"Name\"] for metric in training_metric_definitions]\n",
    "\n",
    "metrics_df = {\n",
    "    metric_name: df.query(f\"metric_name == '{metric_name}'\") for metric_name in metric_names\n",
    "}\n",
    "\n",
    "metrics_df[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08072894",
   "metadata": {},
   "source": [
    "## 4.6. Deploy & run Inference on the fine-tuned model\n",
    "***\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an image. We follow the same steps as in the [Section 3 - Run inference on the pre-trained model](#3.-Run-inference-on-the-pre-trained-model). We start by retrieving the artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `ic_estimator` that we fine-tuned.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7915265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "#inference_instance_type = \"ml.p2.xlarge\"\n",
    "#inference_instance_type = \"ml.p3.xlarge\"\n",
    "inference_instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-FT-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else ic_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc0822a-4ddc-47b8-946a-158b6580bf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow-ic-resnet-50-classification-1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c5c68bb-fe55-43f3-bc05-8a84cf7abe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data in a list\n",
    "import os\n",
    "#jpg_files = [f for f in os.listdir('/root/contrail_processed/Neg') if f.endswith('.JPG')]\n",
    "#jpg_files = [f for f in os.listdir('/root/contrail_processed/Neg/filename00012_archive') if f.endswith('B4.JPG')]\n",
    "jpg_files = [f for f in os.listdir('/root/contrail_processed/Pos/') if f.endswith('B4.JPG')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cc32075-3d03-4874-bf19-5fb66dfff543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LC08_L1TP_008014_20180402_20180416_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_032035_20180410_20180417_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_032042_20180120_20180206_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_021048_20181225_20190129_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_223069_20180927_20181009_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_026015_20180907_20180912_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_020036_20180812_20180828_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_004072_20181015_20181030_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_045030_20180320_20180403_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_079011_20180825_20180829_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_041024_20180815_20180829_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_019045_20180210_20180222_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_008026_20181011_20181030_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_005028_20180803_20180814_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_219074_20180611_20180615_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_061012_20180710_20180717_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_012031_20180905_20180912_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_018016_20180713_20180730_01_T1_B4.JPG',\n",
       " 'LC08_L1TP_221076_20180727_20180731_01_T1_B4.JPG']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ceef6a0-a1d9-4f66-a181-31bd6f9df348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('/root/contrail_processed/Neg/filename00012_archive')\n",
    "os.chdir('/root/contrail_processed/Pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45ef5379-d930-4812-b53d-692f16e9d7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_008014_20180402_20180416_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.922532141, 0.0774678513], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_032035_20180410_20180417_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.458142072, 0.541857958], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_032042_20180120_20180206_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.0482598394, 0.951740146], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_021048_20181225_20190129_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.399149328, 0.600850642], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_223069_20180927_20181009_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.395383596, 0.604616404], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_026015_20180907_20180912_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.472204566, 0.527795494], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_020036_20180812_20180828_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.294558167, 0.705441833], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_004072_20181015_20181030_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.266338021, 0.73366195], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_045030_20180320_20180403_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.681549847, 0.318450153], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_079011_20180825_20180829_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.696739554, 0.303260416], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_041024_20180815_20180829_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.330912977, 0.669087052], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_019045_20180210_20180222_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.793912888, 0.206087112], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_008026_20181011_20181030_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.476284355, 0.523715615], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_005028_20180803_20180814_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.0846118182, 0.915388167], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_219074_20180611_20180615_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.312000871, 0.687999189], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_061012_20180710_20180717_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.345306, 0.65469408], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_012031_20180905_20180912_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.612561226, 0.387438715], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_018016_20180713_20180730_01_T1_B4.JPG Predicted Label: neg Model Predictions: {'probabilities': [0.65186882, 0.34813115], 'labels': ['neg', 'pos'], 'predicted_label': 'neg'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<figcaption>LC08_L1TP_221076_20180727_20180731_01_T1_B4.JPG Predicted Label: pos Model Predictions: {'probabilities': [0.0869820938, 0.913017929], 'labels': ['neg', 'pos'], 'predicted_label': 'pos'}</figcaption>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "for filename in jpg_files:  #contrail_processed/Pos/LC08_L1TP_020026_20180217_20180307_01_T1_B10.JPG\n",
    "    with open(filename, \"rb\") as file:\n",
    "        img = file.read()\n",
    "    query_response = finetuned_predictor.predict(\n",
    "        img, {\"ContentType\": \"application/x-image\", \"Accept\": \"application/json;verbose\"}\n",
    "    )\n",
    "    model_predictions = json.loads(query_response)\n",
    "    predicted_label = model_predictions[\"predicted_label\"]\n",
    "    display(\n",
    "        HTML(\n",
    "    #        f'<img src={image_filename} alt={image_filename} align=\"left\" style=\"width: 250px;\"/>'\n",
    "#            f'<img src=filename alt=filename align=\"left\" style=\"width: 250px;\"/>'     \n",
    "#            f\"<figcaption>{filename} Predicted Label: {predicted_label}</figcaption>\"\n",
    "            f\"<figcaption>{filename} Predicted Label: {predicted_label} Model Predictions: {model_predictions}</figcaption>\"\n",
    "        )\n",
    "    )\n",
    "    query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a21fe029-84fb-4b5c-9e27-1f98d19b9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/root/contrail_processed/Pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924980b5-456b-42a6-a656-fb9023b8c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aaec0cf8-633a-4f43-b07b-fe61a7e2074b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probabilities': [0.0869820938, 0.913017929],\n",
       " 'labels': ['neg', 'pos'],\n",
       " 'predicted_label': 'pos'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c672f19",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we clean up the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae6c5d",
   "metadata": {},
   "source": [
    "## 4.7. Incrementally train the fine-tuned model\n",
    "\n",
    "***\n",
    "Incremental training allows you to train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance. You can use the artifacts from an existing model and use an expanded dataset to train a new model. Incremental training saves both time and resources as you donâ€™t need to retrain a model from scratch.\n",
    "\n",
    "One may use any dataset (old or new) as long as the dataset format remain the same (set of classes). Incremental training step is similar to the finetuning step discussed above with the following difference: In fine-tuning above, we start with a pre-trained model whereas in incremental training, we start with an existing fine-tuned model.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the previously trained model path based on the output location where artifacts are stored previously and the training job name.\n",
    "\n",
    "\n",
    "if use_amt:  # If using amt, select the model for the best training job.\n",
    "    sage_client = boto3.Session().client(\"sagemaker\")\n",
    "    tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(\n",
    "        HyperParameterTuningJobName=hp_tuner._current_job_name\n",
    "    )\n",
    "    last_training_job_name = tuning_job_result[\"BestTrainingJob\"][\"TrainingJobName\"]\n",
    "else:\n",
    "    last_training_job_name = ic_estimator._current_job_name\n",
    "\n",
    "last_trained_model_path = f\"{s3_output_location}/{last_training_job_name}/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d48880",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_train_output_prefix = \"jumpstart-example-ic-incremental-training\"\n",
    "\n",
    "incremental_s3_output_location = f\"s3://{output_bucket}/{incremental_train_output_prefix}/output\"\n",
    "\n",
    "incremental_training_job_name = name_from_base(f\"jumpstart-example-{model_id}-incremental-training\")\n",
    "\n",
    "incremental_train_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=last_trained_model_path,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=incremental_s3_output_location,\n",
    "    base_job_name=incremental_training_job_name,\n",
    "    metric_definitions=training_metric_definitions,\n",
    ")\n",
    "\n",
    "incremental_train_estimator.fit({\"training\": training_dataset_s3_path}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb937a0",
   "metadata": {},
   "source": [
    "Once trained, we can use the same steps as in [Deploy & run Inference on the fine-tuned model](#4.5.-Deploy-&-run-Inference-on-the-fine-tuned-model) to deploy the model."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
